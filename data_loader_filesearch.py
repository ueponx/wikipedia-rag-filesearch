import os
import json
import time
import hashlib
import argparse
from pathlib import Path
from tqdm import tqdm
from google import genai
from google.genai import types
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


def safe_filename(original_filename):
    """æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨ãªASCIIåã«å¤‰æ›
    
    Args:
        original_filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«å
        
    Returns:
        å®‰å…¨ãªASCIIåã®ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    name, ext = os.path.splitext(original_filename)
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ç”¨
    hash_name = hashlib.md5(name.encode('utf-8')).hexdigest()[:16]
    return f"wiki_{hash_name}{ext}"


def load_file_mappings(mapping_file='file_mappings.json'):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        mapping_file: ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
    """
    if os.path.exists(mapping_file):
        with open(mapping_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_file_mappings(mappings, mapping_file='file_mappings.json'):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜
    
    Args:
        mappings: ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
        mapping_file: ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mappings, f, ensure_ascii=False, indent=2)


def get_or_create_store(client, store_name=None):
    """File Search Storeã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
    
    Args:
        client: Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        store_name: æ—¢å­˜ã®Storeåï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
        
    Returns:
        Store object
    """
    if store_name:
        print(f"æ—¢å­˜ã®Storeã‚’ä½¿ç”¨: {store_name}")
        # æ—¢å­˜Storeã‚’ä½¿ç”¨
        class ExistingStore:
            def __init__(self, name):
                self.name = name
        return ExistingStore(store_name)
    else:
        # æ–°ã—ã„Storeã‚’ä½œæˆ
        print("æ–°ã—ã„File Search Storeã‚’ä½œæˆä¸­...")
        store = client.file_search_stores.create(
            config={'display_name': 'wikipedia-knowledge-base'}
        )
        print(f"Storeä½œæˆå®Œäº†: {store.name}")
        print("\n" + "=" * 70)
        print("ğŸ’¡ ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„:")
        print(f"STORE_NAME={store.name}")
        print("=" * 70 + "\n")
        return store


def delete_store_files(client, store_name, mapping_file='file_mappings.json'):
    """Storeå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ
    
    Note: File Search APIã§ã¯Storeå†…ã®å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ãŒå›°é›£ãªãŸã‚ã€
    ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®ã¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚Storeè‡ªä½“ã‚’å‰Šé™¤ã™ã‚‹å ´åˆã¯ã€
    .envã®STORE_NAMEã‚’å‰Šé™¤ã—ã¦æ–°è¦ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    Args:
        client: Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        store_name: Storeå
        mapping_file: ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    try:
        print("ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’ã‚¯ãƒªã‚¢ä¸­...")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        if os.path.exists(mapping_file):
            os.remove(mapping_file)
            print(f"{mapping_file}ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        print("\nâš ï¸ æ³¨æ„: Storeå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("å®Œå…¨ã«ãƒªã‚»ãƒƒãƒˆã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("1. .envãƒ•ã‚¡ã‚¤ãƒ«ã®STORE_NAMEã‚’å‰Šé™¤ã¾ãŸã¯ç©ºã«ã™ã‚‹")
        print("2. data_loader_filesearch.pyã‚’å†å®Ÿè¡Œï¼ˆæ–°ã—ã„StoreãŒä½œæˆã•ã‚Œã¾ã™ï¼‰")
        print(f"\nç¾åœ¨ã®Storeå: {store_name}")
        
    except Exception as e:
        print(f"ãƒªã‚»ãƒƒãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


def upload_wikipedia_data(data_dir, reset=False, mapping_file='file_mappings.json'):
    """Wikipediaãƒ‡ãƒ¼ã‚¿ã‚’File Search Storeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    
    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        reset: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã‹
        mapping_file: ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
    client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
    store_name = os.getenv("STORE_NAME")
    
    # Storeã®å–å¾—ã¾ãŸã¯ä½œæˆ
    if reset and store_name:
        confirm = input("æ—¢å­˜ã®Storeã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if confirm.lower() == 'y':
            # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
            delete_store_files(client, store_name, mapping_file)
            print("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ\n")
    
    store = get_or_create_store(client, store_name)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    data_path = Path(data_dir)
    if not data_path.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{data_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # markdownãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    md_files = list(data_path.glob("*.md"))
    
    if not md_files:
        print(f"{data_dir} ã«markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"{len(md_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿
    mappings = load_file_mappings(mapping_file)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    success_count = 0
    error_count = 0
    
    for file_path in tqdm(md_files, desc="ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­"):
        try:
            original_name = file_path.name
            ascii_name = safe_filename(original_name)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆASCIIåï¼‰
            temp_path = file_path.parent / ascii_name
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆASCIIåã§ï¼‰
            import shutil
            shutil.copy2(file_path, temp_path)
            
            try:
                tqdm.write(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­: {original_name} -> {ascii_name}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                operation = client.file_search_stores.upload_to_file_search_store(
                    file_search_store_name=store.name,
                    file=str(temp_path),
                    config={
                        'display_name': file_path.stem,
                    }
                )
                
                # å®Œäº†å¾…æ©Ÿï¼ˆå…¬å¼æ¨å¥¨: 5ç§’é–“éš”ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 120ç§’ï¼‰
                timeout = 120
                start_time = time.time()
                while not operation.done:
                    if time.time() - start_time > timeout:
                        raise TimeoutError("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                    time.sleep(5)  # å…¬å¼æ¨å¥¨ã®å¾…æ©Ÿæ™‚é–“
                    operation = client.operations.get(operation)
                
                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ
                tqdm.write(f"  âœ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {original_name}")
                
                # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                mappings[ascii_name] = {
                    'original_filename': original_name,
                    'title': file_path.stem,
                    'upload_date': time.strftime('%Y-%m-%dT%H:%M:%S'),
                    'operation_name': operation.name if hasattr(operation, 'name') else 'N/A',
                    'file_size': file_path.stat().st_size
                }
                
                success_count += 1
                
            except Exception as upload_error:
                error_count += 1
                tqdm.write(f"  âœ— ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({original_name}): {upload_error}")
                
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if temp_path.exists():
                    temp_path.unlink()
            
        except Exception as e:
            error_count += 1
            tqdm.write(f"\nå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({file_path.name}): {e}")
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®ä¿å­˜
    save_file_mappings(mappings, mapping_file)
    
    # çµæœã®è¡¨ç¤º
    print(f"\nå®Œäº†: {success_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    if error_count > 0:
        print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‹ã‚‰ç·æ•°ã‚’è¡¨ç¤º
    print(f"File Search Storeç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(mappings)}ä»¶ï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚ˆã‚Šï¼‰")
    
    if len(mappings) > 0:
        print("\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«:")
        for i, (ascii_name, info) in enumerate(list(mappings.items())[:5], 1):
            original = info.get('original_filename', 'N/A')
            title = info.get('title', 'N/A')
            print(f"  {i}. {title} ({original})")
        if len(mappings) > 5:
            print(f"  ... ä»– {len(mappings) - 5}ä»¶")
        print("\nâœ“ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
        print("  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãŒå®Œäº†ã™ã‚‹ã¾ã§ã€æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")
    
    print(f"\nãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {mapping_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='Wikipediaè¨˜äº‹ã‚’File Search Storeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰'
    )
    parser.add_argument(
        '--data-dir',
        default='./data/wikipedia',
        help='Wikipediaã®markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./data/wikipedia)'
    )
    parser.add_argument(
        '--reset',
        action='store_true',
        help='æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰'
    )
    parser.add_argument(
        '--mapping-file',
        default='file_mappings.json',
        help='ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜å…ˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: file_mappings.json)'
    )
    
    args = parser.parse_args()
    
    print("=== Wikipedia RAG File Search ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ ===\n")
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.data_dir}\n")
    
    # ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_wikipedia_data(args.data_dir, args.reset, args.mapping_file)


if __name__ == "__main__":
    main()